{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"../images/DLI_Header.png\" alt=\"標頭\" style=\"width: 400px;\"/> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 測驗：作者歸屬"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "作者歸屬是一種文字分類問題。和疾病文字分類問題不同，目標不是用 _主題_，而是用 _作者_ 來分類文字。\n",
    "\n",
    "解決此類問題時，潛在假設為探討的作者 *皆有不同風格*，*且可透過模型分辨*。這對 BERT 等模型來說是否如此呢？語言模型是否能夠「瞭解」寫作風格？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 目錄\n",
    "[問題](#問題)<br>\n",
    "[分數](#分數)<br>\n",
    "[步驟 1：準備資料](#步驟-1：準備資料)<br>\n",
    "[步驟 2：準備模型設定](#步驟-2：準備模型設定)<br>\n",
    "[步驟 3：準備訓練工具設定](#步驟-3：準備訓練工具設定)<br>\n",
    "[步驟 4：訓練](#步驟-4：訓練)<br>\n",
    "[步驟 5：推論](#步驟-5：推論)<br>\n",
    "[步驟 6：提交測驗](#步驟-6：提交測驗)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 問題\n",
    "### 《聯邦黨人文集 (The Federalist Papers) 》：歷史謎題！\n",
    "\n",
    "[《聯邦黨人文集 (The Federalist Papers) 》](https://en.wikipedia.org/wiki/The_Federalist_Papers)是一系列在 1787 年至 1788 年之間，由 [Alexander Hamilton](https://en.wikipedia.org/wiki/Alexander_Hamilton)、[James Madison](https://en.wikipedia.org/wiki/James_Madison) 與 [John Jay](https://en.wikipedia.org/wiki/John_Jay) 撰寫的文章。他們最初以假名「Publius」發表文章，旨在推動批准當時的美國新憲法。之後出現了一份清單，註明全部 85 篇文章的作者分別是誰。然而，部分文章的作者身分依然存疑。《聯邦黨人文集》的作者歸屬問題是過去統計自然語言處理 (NLP) 研究的熱門議題。現在，您將嘗試用自己的 BERT 專案模型解決問題。\n",
    "<img style=\"float: right;\" src=\"images/HandM.png\" width=400>\n",
    "                                                                                                           \n",
    "具體而言，問題就是要判斷每一篇作者身分有爭議的文章，究竟是由 Alexander Hamilton 還是 James Madison 撰寫而成。在此練習中，您可以假設每篇文章都只有一位作者，也就是說，並沒有合著的情況 (雖然我們並非完全肯定*這點*！)，以及每位作者都有明確的寫作風格，並在所有作者身分不明的文章中一致展現。\n",
    "\n",
    "### 您的專案\n",
    "在此專案中，您將獲得標記為 `train.tsv` 與 `dev.tsv` 的資料集。共有 10 組測試集，每個測試集都是一篇作者身分有爭議的文章。`data/federalist_papers_HM` 目錄包含全部資料集。\n",
    "\n",
    "每個「句子」實際上是由多個句子組成，每組句子總字數約 256 字。標籤「0」代表 HAMILTON，「1」則是 MADISON。在範例檔案中，Hamilton 撰寫的文章數量超過 Madison。在建立驗證集時，我們已確保兩個標籤的分布與訓練集大致相同。\n",
    "\n",
    "您的任務是使用 NeMo 建立神經網路，如實驗 2 的做法。您將訓練並測試模型。接著使用隨附的定序代碼，看看您的模型為此「歷史謎題」提出什麼解答！\n",
    "\n",
    "過程中，您也會儲存程式碼片段，以在完成之後使用自動評分工具加以測試。本實作筆記的結尾附有此部分的提交說明。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 分數\n",
    "我們將就您建立與訓練專案模型的能力，而非最終結果評分。此編碼測驗共佔 70 分，評分項目如下：\n",
    "\n",
    "### 評分指標\n",
    "\n",
    "| 步驟 | 計分 | 修正此處？ | 點數 |\n",
    "|--------------------------------------|-----------------------------------------------------------|----------|--------|\n",
    "| 1.準備專案 | 修正資料格式 (正確格式) | 2 | 10 |\n",
    "| 2.準備模型設定 | 設定要用來覆寫的模型參數 | 3 | 15 |\n",
    "| 3.準備訓練工具設定 | 設定要用來覆寫的訓練工具參數 | 3 | 15 |\n",
    "| 4.訓練 | 執行訓練工具 (顯示訓練正確的訓練記錄) | 4 | 20 |\n",
    "| 5.推論 | 執行推論 (顯示目前專案的結果) | 0 | 10 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "雖然您目前應該已很擅長在無人協助下建立專案，我們還是提供了一些基本架構，包含變數的特定名稱。這是為了方便使用自動評分工具檢測，因此請在測驗中使用這些架構。此外，本測驗使用 `text_classification_with_bert.py` 指令碼和設定檔案覆寫來測您的命令列方法使用狀況。您可以自行變更如模型名稱、序列長度、批次大小、學習率與 Epoch 數量等參數，在您認為適當之處改善模型。\n",
    "\n",
    "若您確信已打造出可靠模型，請遵循實作筆記結尾的說明提交測驗。\n",
    "\n",
    "### 資源與提示\n",
    "* **範例程式碼：**<br>\n",
    "您可以在左側的檔案瀏覽器中找到 `lab2_reference_notebooks` 目錄。其中包含實驗室 2 的文字分類與命名實體識別解決方案實作筆記，供您參考。\n",
    "* **語言模型 (PRETRAINED_MODEL_NAME):**<br>\n",
    "您可能會發現嘗試使用不同的語言模型有助判斷風格。確切來說，大寫可能很重要，因此您也許會想嘗試看看「大小寫」模型。\n",
    "* **序列長度上限 (MAX_SEQ_LEN)：**<br>\n",
    "可用於 MAX_SEQ_LENGTH 的值為 64、128 或 256。較大的模型 (BERT-large、Megatron) 可能需要使用較小的 MAX_SEQ_LENGTH，以免發生記憶體不足的錯誤。\n",
    "* **類別數量 (NUM_CLASSES)：**<br>\n",
    "我們僅需探討《聯邦黨人文集》中由 HAMILTON 與 MADISON 所撰寫的文章，資料集已排除 John Jay 撰寫的文章。\n",
    "* **批次大小 (BATCH_SIZE)：**<br>\n",
    "批次大小越大，訓練速度越快，但大規模語言模型通常會很快耗盡可用記憶體。\n",
    "* **記憶體使用率：**<br>\n",
    "有些模型規模相當大。如果在訓練時出現「執行階段錯誤：CUDA 記憶體不足」，就表示您需要降低批次大小、序列長度及/或選擇較小的語言模型、重新啟動核心，並從實作筆記的開頭重新來過。\n",
    "* **精確度和損失率：**<br>\n",
    "此專案絕對能達到 95% 以上的模型精確度。訓練模型時，除了要注意精確度變化，也需要注意損失率值。降低損失率，且讓損失率值越低越好，才能取得最佳結果。\n",
    "* **Epoch 數量 (NUM_EPOCHS)：**<br>\n",
    "您的模型可能需要執行更多 Epoch (也可能不需要！)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 步驟 1：準備資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import useful utilities for grading\n",
    "import os\n",
    "import json\n",
    "import glob\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "def get_latest_model():  \n",
    "    nemo_model_paths = glob.glob('nemo_experiments/TextClassification/*/checkpoints/*.nemo')\n",
    "    # Sort newest first\n",
    "    nemo_model_paths.sort(reverse=True)\n",
    "    return nemo_model_paths[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "資料已在資料目錄中，請參閱下列儲存格內的清單："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev.tsv   test49.tsv  test51.tsv  test53.tsv  test55.tsv  test57.tsv  train.tsv\n",
      "test.tsv  test50.tsv  test52.tsv  test54.tsv  test56.tsv  test62.tsv\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = '/dli/task/data/federalist_papers_HM'\n",
    "!ls $DATA_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 資料格式 (計分)\n",
    "資料格式並非執行 NeMo 文字分類所需的正確格式。修正資料，並在 DATA_DIR 中將新資料集儲存為 `train_nemo_format.tsv` 與 `dev_nemo_format.tsv`。您不必對測試檔案執行任何動作。\n",
    "\n",
    "完成 <i><strong style=\"color:green;\">#FIXME</strong></i> 行，然後執行已儲存的儲存格。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correct the format for train.tsv and dev.tsv\n",
    "#   and save the updates in train_nemo_format.tsv and dev_nemo_format.tsv\n",
    "!sed 1d $DATA_DIR/train.tsv > $DATA_DIR/train_nemo_format.tsv\n",
    "!sed 1d $DATA_DIR/dev.tsv > $DATA_DIR/dev_nemo_format.tsv\n",
    "#FIXME train.tsv format\n",
    "#FIXME dev.tsv format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****\n",
      "train_nemo_format.tsv sample\n",
      "*****\n",
      "Concerning Dangers from Dissensions Between the States For the Independent Journal .To the People of the State of New York : THE three last numbers of this paper have been dedicated to an enumeration of the dangers to which we should be exposed , in a state of disunion , from the arms and arts of foreign nations .I shall now proceed to delineate dangers of a different and , perhaps , still more alarming kind -- those which will in all probability flow from dissensions between the States themselves , and from domestic factions and convulsions .These have been already in some instances slightly anticipated ; but they deserve a more particular and more full investigation .A man must be far gone in Utopian speculations who can seriously doubt that , if these States should either be wholly disunited , or only united in partial confederacies , the subdivisions into which they might be thrown would have frequent and violent contests with each other .To presume a want of motives for such contests as an argument against their existence , would be to forget that men are ambitious , vindictive , and rapacious .To look for a continuation of harmony between a number of independent , unconnected sovereignties in the same neighborhood , would be to disregard the uniform course of human events , and to set at defiance the accumulated experience of ages .The causes of hostility among nations are innumerable .\t0\n",
      "There are some which have a general and almost constant operation upon the collective bodies of society .Of this description are the love of power or the desire of pre-eminence and dominion -- the jealousy of power , or the desire of equality and safety .There are others which have a more circumscribed though an equally operative influence within their spheres .Such are the rivalships and competitions of commerce between commercial nations .And there are others , not less numerous than either of the former , which take their origin entirely in private passions ; in the attachments , enmities , interests , hopes , and fears of leading individuals in the communities of which they are members .Men of this class , whether the favorites of a king or of a people , have in too many instances abused the confidence they possessed ; and assuming the pretext of some public motive , have not scrupled to sacrifice the national tranquillity to personal advantage or personal gratification .The celebrated Pericles , in compliance with the resentment of a prostitute,1 at the expense of much of the blood and treasure of his countrymen , attacked , vanquished , and destroyed the city of the SAMNIANS .The same man , stimulated by private pique against the MEGARENSIANS,2 another nation of Greece , or to avoid a prosecution with which he was threatened as an accomplice of a supposed theft of the statuary Phidias,3 or to get rid of the accusations prepared to be brought against him for dissipating the funds of the state in the purchase of popularity,4 or from a combination of all these causes , was the primitive author of that famous and fatal war , distinguished in the Grecian annals by the name of the PELOPONNESIAN war ; which , after various vicissitudes , intermissions , and renewals , terminated in the ruin of the Athenian commonwealth .\t0\n",
      "The ambitious cardinal , who was prime minister to Henry VIII. , permitting his vanity to aspire to the triple crown,5 entertained hopes of succeeding in the acquisition of that splendid prize by the influence of the Emperor Charles V. To secure the favor and interest of this enterprising and powerful monarch , he precipitated England into a war with France , contrary to the plainest dictates of policy , and at the hazard of the safety and independence , as well of the kingdom over which he presided by his counsels , as of Europe in general .For if there ever was a sovereign who bid fair to realize the project of universal monarchy , it was the Emperor Charles V. , of whose intrigues Wolsey was at once the instrument and the dupe .The influence which the bigotry of one female,6 the petulance of another,7 and the cabals of a third,8 had in the contemporary policy , ferments , and pacifications , of a considerable part of Europe , are topics that have been too often descanted upon not to be generally known .To multiply examples of the agency of personal considerations in the production of great national events , either foreign or domestic , according to their direction , would be an unnecessary waste of time .Those who have but a superficial acquaintance with the sources from which they are to be drawn , will themselves recollect a variety of instances ; and those who have a tolerable knowledge of human nature will not stand in need of such lights to form their opinion either of the reality or extent of that agency .\t0\n",
      "\n",
      "\n",
      "*****\n",
      "dev_nemo_format.tsv sample\n",
      "*****\n",
      "There have been , if I may so express it , almost as many popular as royal wars .The cries of the nation and the importunities of their representatives have , upon various occasions , dragged their monarchs into war , or continued them in it , contrary to their inclinations , and sometimes contrary to the real interests of the State .In that memorable struggle for superiority between the rival houses of AUSTRIA and BOURBON , which so long kept Europe in a flame , it is well known that the antipathies of the English against the French , seconding the ambition , or rather the avarice , of a favorite leader,10 protracted the war beyond the limits marked out by sound policy , and for a considerable time in opposition to the views of the court .The wars of these two last-mentioned nations have in a great measure grown out of commercial considerations , -- the desire of supplanting and the fear of being supplanted , either in particular branches of traffic or in the general advantages of trade and navigation .From this summary of what has taken place in other countries , whose situations have borne the nearest resemblance to our own , what reason can we have to confide in those reveries which would seduce us into an expectation of peace and cordiality between the members of the present confederacy , in a state of separation ? Have we not already seen enough of the fallacy and extravagance of those idle theories which have amused us with promises of an exemption from the imperfections , weaknesses and evils incident to society in every shape ?\t0\n",
      "They would , at the same time , be necessitated to strengthen the executive arm of government , in doing which their constitutions would acquire a progressive direction toward monarchy .It is of the nature of war to increase the executive at the expense of the legislative authority .The expedients which have been mentioned would soon give the States or confederacies that made use of them a superiority over their neighbors .Small states , or states of less natural strength , under vigorous governments , and with the assistance of disciplined armies , have often triumphed over large states , or states of greater natural strength , which have been destitute of these advantages .Neither the pride nor the safety of the more important States or confederacies would permit them long to submit to this mortifying and adventitious superiority .They would quickly resort to means similar to those by which it had been effected , to reinstate themselves in their lost pre-eminence .Thus , we should , in a little time , see established in every part of this country the same engines of despotism which have been the scourge of the Old World .This , at least , would be the natural course of things ; and our reasonings will be the more likely to be just , in proportion as they are accommodated to this standard .These are not vague inferences drawn from supposed or speculative defects in a Constitution , the whole power of which is lodged in the hands of a people , or their representatives and delegates , but they are solid conclusions , drawn from the natural and necessary progress of human affairs .\t0\n",
      "Such a point gained from the British government , and which could not be expected without an equivalent in exemptions and immunities in our markets , would be likely to have a correspondent effect on the conduct of other nations , who would not be inclined to see themselves altogether supplanted in our trade .A further resource for influencing the conduct of European nations toward us , in this respect , would arise from the establishment of a federal navy .There can be no doubt that the continuance of the Union under an efficient government would put it in our power , at a period not very distant , to create a navy which , if it could not vie with those of the great maritime powers , would at least be of respectable weight if thrown into the scale of either of two contending parties .This would be more peculiarly the case in relation to operations in the West Indies .A few ships of the line , sent opportunely to the reinforcement of either side , would often be sufficient to decide the fate of a campaign , on the event of which interests of the greatest magnitude were suspended .Our position is , in this respect , a most commanding one .And if to this consideration we add that of the usefulness of supplies from this country , in the prosecution of military operations in the West Indies , it will readily be perceived that a situation so favorable would enable us to bargain with great advantage for commercial privileges .\t0\n"
     ]
    }
   ],
   "source": [
    "# check your work\n",
    "print(\"*****\\ntrain_nemo_format.tsv sample\\n*****\")\n",
    "!head -n 3 $DATA_DIR/train_nemo_format.tsv\n",
    "print(\"\\n\\n*****\\ndev_nemo_format.tsv sample\\n*****\")\n",
    "!head -n 3 $DATA_DIR/dev_nemo_format.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run to save for assessment- DO NOT CHANGE\n",
    "import os.path\n",
    "DATA_DIR = '/dli/task/data/federalist_papers_HM'\n",
    "step1 = []\n",
    "try:\n",
    "    with open(os.path.join(DATA_DIR,'train_nemo_format.tsv')) as f:\n",
    "        content = f.readlines()\n",
    "        step1 += content[:2]\n",
    "    with open(os.path.join(DATA_DIR,'dev_nemo_format.tsv')) as f:\n",
    "        content = f.readlines()\n",
    "        step1 += content[:2]\n",
    "except:\n",
    "    pass\n",
    "                \n",
    "with open(\"my_assessment/step1.json\", \"w\") as outfile: \n",
    "    json.dump(step1, outfile) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 步驟 2：準備模型設定\n",
    "檢閱預設模型設定與可用的語言模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nemo_path: text_classification_model.nemo\n",
      "tokenizer:\n",
      "  tokenizer_name: ${model.language_model.pretrained_model_name}\n",
      "  vocab_file: null\n",
      "  tokenizer_model: null\n",
      "  special_tokens: null\n",
      "language_model:\n",
      "  pretrained_model_name: bert-base-uncased\n",
      "  lm_checkpoint: null\n",
      "  config_file: null\n",
      "  config: null\n",
      "classifier_head:\n",
      "  num_output_layers: 2\n",
      "  fc_dropout: 0.1\n",
      "class_labels:\n",
      "  class_labels_file: null\n",
      "dataset:\n",
      "  num_classes: ???\n",
      "  do_lower_case: false\n",
      "  max_seq_length: 256\n",
      "  class_balancing: null\n",
      "  use_cache: false\n",
      "train_ds:\n",
      "  file_path: null\n",
      "  batch_size: 64\n",
      "  shuffle: true\n",
      "  num_samples: -1\n",
      "  num_workers: 3\n",
      "  drop_last: false\n",
      "  pin_memory: false\n",
      "validation_ds:\n",
      "  file_path: null\n",
      "  batch_size: 64\n",
      "  shuffle: false\n",
      "  num_samples: -1\n",
      "  num_workers: 3\n",
      "  drop_last: false\n",
      "  pin_memory: false\n",
      "test_ds:\n",
      "  file_path: null\n",
      "  batch_size: 64\n",
      "  shuffle: false\n",
      "  num_samples: -1\n",
      "  num_workers: 3\n",
      "  drop_last: false\n",
      "  pin_memory: false\n",
      "optim:\n",
      "  name: adam\n",
      "  lr: 2.0e-05\n",
      "  betas:\n",
      "  - 0.9\n",
      "  - 0.999\n",
      "  weight_decay: 0.01\n",
      "  sched:\n",
      "    name: WarmupAnnealing\n",
      "    warmup_steps: null\n",
      "    warmup_ratio: 0.1\n",
      "    last_epoch: -1\n",
      "    monitor: val_loss\n",
      "    reduce_on_plateau: false\n",
      "infer_samples:\n",
      "- by the end of no such thing the audience , like beatrice , has a watchful affection\n",
      "  for the monster .\n",
      "- director rob marshall went out gunning to make a great one .\n",
      "- uneasy mishmash of styles and genres .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Take a look at the default model portion of the config file\n",
    "CONFIG_DIR = \"/dli/task/nemo/examples/nlp/text_classification/conf\"\n",
    "CONFIG_FILE = \"text_classification_config.yaml\"\n",
    "\n",
    "config = OmegaConf.load(CONFIG_DIR + \"/\" + CONFIG_FILE)\n",
    "print(OmegaConf.to_yaml(config.model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['megatron-bert-345m-uncased',\n",
       " 'megatron-bert-345m-cased',\n",
       " 'megatron-bert-uncased',\n",
       " 'megatron-bert-cased',\n",
       " 'biomegatron-bert-345m-uncased',\n",
       " 'biomegatron-bert-345m-cased',\n",
       " 'bert-base-uncased',\n",
       " 'bert-large-uncased',\n",
       " 'bert-base-cased',\n",
       " 'bert-large-cased',\n",
       " 'bert-base-multilingual-uncased',\n",
       " 'bert-base-multilingual-cased',\n",
       " 'bert-base-chinese',\n",
       " 'bert-base-german-cased',\n",
       " 'bert-large-uncased-whole-word-masking',\n",
       " 'bert-large-cased-whole-word-masking',\n",
       " 'bert-large-uncased-whole-word-masking-finetuned-squad',\n",
       " 'bert-large-cased-whole-word-masking-finetuned-squad',\n",
       " 'bert-base-cased-finetuned-mrpc',\n",
       " 'bert-base-german-dbmdz-cased',\n",
       " 'bert-base-german-dbmdz-uncased',\n",
       " 'cl-tohoku/bert-base-japanese',\n",
       " 'cl-tohoku/bert-base-japanese-whole-word-masking',\n",
       " 'cl-tohoku/bert-base-japanese-char',\n",
       " 'cl-tohoku/bert-base-japanese-char-whole-word-masking',\n",
       " 'TurkuNLP/bert-base-finnish-cased-v1',\n",
       " 'TurkuNLP/bert-base-finnish-uncased-v1',\n",
       " 'wietsedv/bert-base-dutch-cased',\n",
       " 'distilbert-base-uncased',\n",
       " 'distilbert-base-uncased-distilled-squad',\n",
       " 'distilbert-base-cased',\n",
       " 'distilbert-base-cased-distilled-squad',\n",
       " 'distilbert-base-german-cased',\n",
       " 'distilbert-base-multilingual-cased',\n",
       " 'distilbert-base-uncased-finetuned-sst-2-english',\n",
       " 'roberta-base',\n",
       " 'roberta-large',\n",
       " 'roberta-large-mnli',\n",
       " 'distilroberta-base',\n",
       " 'roberta-base-openai-detector',\n",
       " 'roberta-large-openai-detector',\n",
       " 'albert-base-v1',\n",
       " 'albert-large-v1',\n",
       " 'albert-xlarge-v1',\n",
       " 'albert-xxlarge-v1',\n",
       " 'albert-base-v2',\n",
       " 'albert-large-v2',\n",
       " 'albert-xlarge-v2',\n",
       " 'albert-xxlarge-v2']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See what BERT-like language models are available\n",
    "from nemo.collections import nlp as nemo_nlp\n",
    "nemo_nlp.modules.get_pretrained_lm_models_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 設定參數 (計分)\n",
    "完成 <i><strong style=\"color:green;\">#FIXME</strong></i> 行，然後執行已儲存的儲存格。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the values\n",
    "NUM_CLASSES = 2\n",
    "MAX_SEQ_LENGTH = 128\n",
    "BATCH_SIZE = 16\n",
    "PATH_TO_TRAIN_FILE = \"/dli/task/data/federalist_papers_HM/train_nemo_format.tsv\"\n",
    "PATH_TO_VAL_FILE = \"/dli/task/data/federalist_papers_HM/dev_nemo_format.tsv\"\n",
    "PRETRAINED_MODEL_NAME = 'bert-base-uncased' # change as desired\n",
    "LR = 1e-4 # change as desired"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run to save for assessment- DO NOT CHANGE\n",
    "with open(\"my_assessment/step2.json\", \"w\") as outfile: \n",
    "    json.dump([MAX_SEQ_LENGTH, NUM_CLASSES, BATCH_SIZE], outfile) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 步驟 3：準備訓練工具設定\n",
    "檢閱預設訓練工具與 exp_manager 設定。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpus: 1\n",
      "num_nodes: 1\n",
      "max_epochs: 100\n",
      "max_steps: null\n",
      "accumulate_grad_batches: 1\n",
      "gradient_clip_val: 0.0\n",
      "amp_level: O0\n",
      "precision: 32\n",
      "accelerator: ddp\n",
      "log_every_n_steps: 1\n",
      "val_check_interval: 1.0\n",
      "resume_from_checkpoint: null\n",
      "num_sanity_val_steps: 0\n",
      "checkpoint_callback: false\n",
      "logger: false\n",
      "\n",
      "exp_dir: null\n",
      "name: TextClassification\n",
      "create_tensorboard_logger: true\n",
      "create_checkpoint_callback: true\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(OmegaConf.to_yaml(config.trainer))\n",
    "print(OmegaConf.to_yaml(config.exp_manager))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 設定參數 (計分)\n",
    "將自動混合精度設為層級 1 的 FP16 精度。將 MAX_EPOCHS 設定在合理層級，可能介於 5 至 20。<br>完成 <i><strong style=\"color:green;\">#FIXME</strong></i> 行，然後執行已儲存的儲存格。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the values\n",
    "MAX_EPOCHS = 10\n",
    "AMP_LEVEL = 'O1'\n",
    "PRECISION = 16 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run to save for assessment - DO NOT CHANGE\n",
    "with open(\"my_assessment/step3.json\", \"w\") as outfile: \n",
    "    json.dump([MAX_EPOCHS, AMP_LEVEL, PRECISION], outfile) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "# 步驟 4：訓練"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 執行訓練工具 (計分)\n",
    "在下列訓練與驗證批次大小、AMP 層級與精度儲存格中完成 <i><strong style=\"color:green;\">#FIXME</strong></i>。然後訓練並執行已儲存的儲存格！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-04-24 01:35:30 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/omegaconf/basecontainer.py:225: UserWarning: cfg.pretty() is deprecated and will be removed in a future version.\n",
      "    Use OmegaConf.to_yaml(cfg)\n",
      "    \n",
      "      warnings.warn(\n",
      "    \n",
      "[NeMo I 2024-04-24 01:35:30 text_classification_with_bert:110] \n",
      "    Config Params:\n",
      "    trainer:\n",
      "      gpus: 1\n",
      "      num_nodes: 1\n",
      "      max_epochs: 10\n",
      "      max_steps: null\n",
      "      accumulate_grad_batches: 1\n",
      "      gradient_clip_val: 0.0\n",
      "      amp_level: O1\n",
      "      precision: 16\n",
      "      accelerator: ddp\n",
      "      log_every_n_steps: 1\n",
      "      val_check_interval: 1.0\n",
      "      resume_from_checkpoint: null\n",
      "      num_sanity_val_steps: 0\n",
      "      checkpoint_callback: false\n",
      "      logger: false\n",
      "    model:\n",
      "      nemo_path: text_classification_model.nemo\n",
      "      tokenizer:\n",
      "        tokenizer_name: ${model.language_model.pretrained_model_name}\n",
      "        vocab_file: null\n",
      "        tokenizer_model: null\n",
      "        special_tokens: null\n",
      "      language_model:\n",
      "        pretrained_model_name: bert-base-uncased\n",
      "        lm_checkpoint: null\n",
      "        config_file: null\n",
      "        config: null\n",
      "      classifier_head:\n",
      "        num_output_layers: 2\n",
      "        fc_dropout: 0.1\n",
      "      class_labels:\n",
      "        class_labels_file: null\n",
      "      dataset:\n",
      "        num_classes: 2\n",
      "        do_lower_case: false\n",
      "        max_seq_length: 128\n",
      "        class_balancing: null\n",
      "        use_cache: false\n",
      "      train_ds:\n",
      "        file_path: /dli/task/data/federalist_papers_HM/train_nemo_format.tsv\n",
      "        batch_size: 16\n",
      "        shuffle: true\n",
      "        num_samples: -1\n",
      "        num_workers: 3\n",
      "        drop_last: false\n",
      "        pin_memory: false\n",
      "      validation_ds:\n",
      "        file_path: /dli/task/data/federalist_papers_HM/dev_nemo_format.tsv\n",
      "        batch_size: 16\n",
      "        shuffle: false\n",
      "        num_samples: -1\n",
      "        num_workers: 3\n",
      "        drop_last: false\n",
      "        pin_memory: false\n",
      "      test_ds:\n",
      "        file_path: null\n",
      "        batch_size: 64\n",
      "        shuffle: false\n",
      "        num_samples: -1\n",
      "        num_workers: 3\n",
      "        drop_last: false\n",
      "        pin_memory: false\n",
      "      optim:\n",
      "        name: adam\n",
      "        lr: 2.0e-05\n",
      "        betas:\n",
      "        - 0.9\n",
      "        - 0.999\n",
      "        weight_decay: 0.01\n",
      "        sched:\n",
      "          name: WarmupAnnealing\n",
      "          warmup_steps: null\n",
      "          warmup_ratio: 0.1\n",
      "          last_epoch: -1\n",
      "          monitor: val_loss\n",
      "          reduce_on_plateau: false\n",
      "      infer_samples: []\n",
      "    exp_manager:\n",
      "      exp_dir: null\n",
      "      name: TextClassification\n",
      "      create_tensorboard_logger: true\n",
      "      create_checkpoint_callback: true\n",
      "    \n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "Using native 16bit precision.\n",
      "[NeMo I 2024-04-24 01:35:30 exp_manager:216] Experiments will be logged at /dli/task/nemo_experiments/TextClassification/2024-04-24_01-35-30\n",
      "[NeMo I 2024-04-24 01:35:30 exp_manager:563] TensorboardLogger has been set up\n",
      "Lock 139819841185008 acquired on /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e.lock\n",
      "Downloading: 100%|█████████████████████████████| 570/570 [00:00<00:00, 1.11MB/s]\n",
      "Lock 139819841185008 released on /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e.lock\n",
      "Lock 139819840859872 acquired on /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99.lock\n",
      "Downloading: 100%|███████████████████████████| 232k/232k [00:00<00:00, 31.1MB/s]\n",
      "Lock 139819840859872 released on /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99.lock\n",
      "Lock 139819841042896 acquired on /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.76ea01b4b85ac16e2cec55c398cba7a943d89ab21dfdd973f6630a152e4b9aed.lock\n",
      "Downloading: 100%|███████████████████████████| 48.0/48.0 [00:00<00:00, 94.3kB/s]\n",
      "Lock 139819841042896 released on /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.76ea01b4b85ac16e2cec55c398cba7a943d89ab21dfdd973f6630a152e4b9aed.lock\n",
      "Lock 139819840859680 acquired on /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4.lock\n",
      "Downloading: 100%|███████████████████████████| 466k/466k [00:00<00:00, 46.4MB/s]\n",
      "Lock 139819840859680 released on /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4.lock\n",
      "Using bos_token, but it is not set yet.\n",
      "Using eos_token, but it is not set yet.\n",
      "[NeMo I 2024-04-24 01:35:30 text_classification_dataset:120] Read 502 examples from /dli/task/data/federalist_papers_HM/train_nemo_format.tsv.\n",
      "[NeMo I 2024-04-24 01:35:31 text_classification_dataset:238] *** Example ***\n",
      "[NeMo I 2024-04-24 01:35:31 text_classification_dataset:239] example 0: ['These', 'Departments', 'Should', 'Not', 'Be', 'So', 'Far', 'Separated', 'as', 'to', 'Have', 'No', 'Constitutional', 'Control', 'Over', 'Each', 'Other', 'From', 'the', 'New', 'York', 'Packet', '.Friday', ',', 'February', '1', ',', '1788', '.To', 'the', 'People', 'of', 'the', 'State', 'of', 'New', 'York', ':', 'IT', 'WAS', 'shown', 'in', 'the', 'last', 'paper', 'that', 'the', 'political', 'apothegm', 'there', 'examined', 'does', 'not', 'require', 'that', 'the', 'legislative', ',', 'executive', ',', 'and', 'judiciary', 'departments', 'should', 'be', 'wholly', 'unconnected', 'with', 'each', 'other', '.I', 'shall', 'undertake', ',', 'in', 'the', 'next', 'place', ',', 'to', 'show', 'that', 'unless', 'these', 'departments', 'be', 'so', 'far', 'connected', 'and', 'blended', 'as', 'to', 'give', 'to', 'each', 'a', 'constitutional', 'control', 'over', 'the', 'others', ',', 'the', 'degree', 'of', 'separation', 'which', 'the', 'maxim', 'requires', ',', 'as', 'essential', 'to', 'a', 'free', 'government', ',', 'can', 'never', 'in', 'practice', 'be', 'duly', 'maintained', '.It', 'is', 'agreed', 'on', 'all', 'sides', ',', 'that', 'the', 'powers', 'properly', 'belonging', 'to', 'one', 'of', 'the', 'departments', 'ought', 'not', 'to', 'be', 'directly', 'and', 'completely', 'administered', 'by', 'either', 'of', 'the', 'other', 'departments', '.It', 'is', 'equally', 'evident', ',', 'that', 'none', 'of', 'them', 'ought', 'to', 'possess', ',', 'directly', 'or', 'indirectly', ',', 'an', 'overruling', 'influence', 'over', 'the', 'others', ',', 'in', 'the', 'administration', 'of', 'their', 'respective', 'powers', '.It', 'will', 'not', 'be', 'denied', ',', 'that', 'power', 'is', 'of', 'an', 'encroaching', 'nature', ',', 'and', 'that', 'it', 'ought', 'to', 'be', 'effectually', 'restrained', 'from', 'passing', 'the', 'limits', 'assigned', 'to', 'it', '.After', 'discriminating', ',', 'therefore', ',', 'in', 'theory', ',', 'the', 'several', 'classes', 'of', 'power', ',', 'as', 'they', 'may', 'in', 'their', 'nature', 'be', 'legislative', ',', 'executive', ',', 'or', 'judiciary', ',', 'the', 'next', 'and', 'most', 'difficult', 'task', 'is', 'to', 'provide', 'some', 'practical', 'security', 'for', 'each', ',', 'against', 'the', 'invasion', 'of', 'the', 'others', '.']\n",
      "[NeMo I 2024-04-24 01:35:31 text_classification_dataset:240] subtokens: [CLS] these departments should not be so far separated as to have no constitutional control over each other from the new york packet . friday , february 1 , 1788 . to the people of the state of new york : it was shown in the last paper that the political ap ##oth ##eg ##m there examined does not require that the legislative , executive , and judiciary departments should be wholly un ##connected with each other . i shall undertake , in the next place , to show that unless these departments be so far connected and blended as to give to each a constitutional control over the others , the degree of separation which the maxim requires , as essential to a free government , can [SEP]\n",
      "[NeMo I 2024-04-24 01:35:31 text_classification_dataset:241] input_ids: 101 2122 7640 2323 2025 2022 2061 2521 5459 2004 2000 2031 2053 6543 2491 2058 2169 2060 2013 1996 2047 2259 14771 1012 5958 1010 2337 1015 1010 15622 1012 2000 1996 2111 1997 1996 2110 1997 2047 2259 1024 2009 2001 3491 1999 1996 2197 3259 2008 1996 2576 9706 14573 13910 2213 2045 8920 2515 2025 5478 2008 1996 4884 1010 3237 1010 1998 14814 7640 2323 2022 12590 4895 24230 2007 2169 2060 1012 1045 4618 16617 1010 1999 1996 2279 2173 1010 2000 2265 2008 4983 2122 7640 2022 2061 2521 4198 1998 19803 2004 2000 2507 2000 2169 1037 6543 2491 2058 1996 2500 1010 1996 3014 1997 8745 2029 1996 20446 5942 1010 2004 6827 2000 1037 2489 2231 1010 2064 102\n",
      "[NeMo I 2024-04-24 01:35:31 text_classification_dataset:242] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "[NeMo I 2024-04-24 01:35:31 text_classification_dataset:243] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "[NeMo I 2024-04-24 01:35:31 text_classification_dataset:244] label: 1\n",
      "[NeMo I 2024-04-24 01:35:31 text_classification_dataset:238] *** Example ***\n",
      "[NeMo I 2024-04-24 01:35:31 text_classification_dataset:239] example 1: ['An', 'entire', 'consolidation', 'of', 'the', 'States', 'into', 'one', 'complete', 'national', 'sovereignty', 'would', 'imply', 'an', 'entire', 'subordination', 'of', 'the', 'parts', ';', 'and', 'whatever', 'powers', 'might', 'remain', 'in', 'them', ',', 'would', 'be', 'altogether', 'dependent', 'on', 'the', 'general', 'will', '.But', 'as', 'the', 'plan', 'of', 'the', 'convention', 'aims', 'only', 'at', 'a', 'partial', 'union', 'or', 'consolidation', ',', 'the', 'State', 'governments', 'would', 'clearly', 'retain', 'all', 'the', 'rights', 'of', 'sovereignty', 'which', 'they', 'before', 'had', ',', 'and', 'which', 'were', 'not', ',', 'by', 'that', 'act', ',', 'EXCLUSIVELY', 'delegated', 'to', 'the', 'United', 'States', '.This', 'exclusive', 'delegation', ',', 'or', 'rather', 'this', 'alienation', ',', 'of', 'State', 'sovereignty', ',', 'would', 'only', 'exist', 'in', 'three', 'cases', ':', 'where', 'the', 'Constitution', 'in', 'express', 'terms', 'granted', 'an', 'exclusive', 'authority', 'to', 'the', 'Union', ';', 'where', 'it', 'granted', 'in', 'one', 'instance', 'an', 'authority', 'to', 'the', 'Union', ',', 'and', 'in', 'another', 'prohibited', 'the', 'States', 'from', 'exercising', 'the', 'like', 'authority', ';', 'and', 'where', 'it', 'granted', 'an', 'authority', 'to', 'the', 'Union', ',', 'to', 'which', 'a', 'similar', 'authority', 'in', 'the', 'States', 'would', 'be', 'absolutely', 'and', 'totally', 'CONTRADICTORY', 'and', 'REPUGNANT', '.I', 'use', 'these', 'terms', 'to', 'distinguish', 'this', 'last', 'case', 'from', 'another', 'which', 'might', 'appear', 'to', 'resemble', 'it', ',', 'but', 'which', 'would', ',', 'in', 'fact', ',', 'be', 'essentially', 'different', ';', 'I', 'mean', 'where', 'the', 'exercise', 'of', 'a', 'concurrent', 'jurisdiction', 'might', 'be', 'productive', 'of', 'occasional', 'interferences', 'in', 'the', 'POLICY', 'of', 'any', 'branch', 'of', 'administration', ',', 'but', 'would', 'not', 'imply', 'any', 'direct', 'contradiction', 'or', 'repugnancy', 'in', 'point', 'of', 'constitutional', 'authority', '.These', 'three', 'cases', 'of', 'exclusive', 'jurisdiction', 'in', 'the', 'federal', 'government', 'may', 'be', 'exemplified', 'by', 'the', 'following', 'instances', ':', 'The', 'last', 'clause', 'but', 'one', 'in', 'the', 'eighth', 'section', 'of', 'the', 'first', 'article', 'provides', 'expressly', 'that', 'Congress', 'shall', 'exercise', '``', 'EXCLUSIVE', 'LEGISLATION', '``', 'over', 'the', 'district', 'to', 'be', 'appropriated', 'as', 'the', 'seat', 'of', 'government', '.']\n",
      "[NeMo I 2024-04-24 01:35:31 text_classification_dataset:240] subtokens: [CLS] an entire consolidation of the states into one complete national sovereignty would imply an entire sub ##ord ##ination of the parts ; and whatever powers might remain in them , would be altogether dependent on the general will . but as the plan of the convention aims only at a partial union or consolidation , the state governments would clearly retain all the rights of sovereignty which they before had , and which were not , by that act , exclusively delegate ##d to the united states . this exclusive delegation , or rather this alien ##ation , of state sovereignty , would only exist in three cases : where the constitution in express terms granted an exclusive authority to the union ; where it granted in [SEP]\n",
      "[NeMo I 2024-04-24 01:35:31 text_classification_dataset:241] input_ids: 101 2019 2972 17439 1997 1996 2163 2046 2028 3143 2120 12601 2052 19515 2019 2972 4942 8551 12758 1997 1996 3033 1025 1998 3649 4204 2453 3961 1999 2068 1010 2052 2022 10462 7790 2006 1996 2236 2097 1012 2021 2004 1996 2933 1997 1996 4680 8704 2069 2012 1037 7704 2586 2030 17439 1010 1996 2110 6867 2052 4415 9279 2035 1996 2916 1997 12601 2029 2027 2077 2018 1010 1998 2029 2020 2025 1010 2011 2008 2552 1010 7580 11849 2094 2000 1996 2142 2163 1012 2023 7262 10656 1010 2030 2738 2023 7344 3370 1010 1997 2110 12601 1010 2052 2069 4839 1999 2093 3572 1024 2073 1996 4552 1999 4671 3408 4379 2019 7262 3691 2000 1996 2586 1025 2073 2009 4379 1999 102\n",
      "[NeMo I 2024-04-24 01:35:31 text_classification_dataset:242] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "[NeMo I 2024-04-24 01:35:31 text_classification_dataset:243] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "[NeMo I 2024-04-24 01:35:31 text_classification_dataset:244] label: 0\n",
      "[NeMo W 2024-04-24 01:35:39 text_classification_dataset:250] Found 502 out of 502 sentences with more than 128 subtokens. Truncated long sentences from the end.\n",
      "[NeMo I 2024-04-24 01:35:39 data_preprocessing:299] Some stats of the lengths of the sequences:\n",
      "[NeMo I 2024-04-24 01:35:39 data_preprocessing:301] Min: 129 |                  Max: 129 |                  Mean: 129.0 |                  Median: 129.0\n",
      "[NeMo I 2024-04-24 01:35:39 data_preprocessing:307] 75 percentile: 129.00\n",
      "[NeMo I 2024-04-24 01:35:39 data_preprocessing:308] 99 percentile: 129.00\n",
      "[NeMo I 2024-04-24 01:35:39 text_classification_dataset:120] Read 115 examples from /dli/task/data/federalist_papers_HM/dev_nemo_format.tsv.\n",
      "[NeMo I 2024-04-24 01:35:39 text_classification_dataset:238] *** Example ***\n",
      "[NeMo I 2024-04-24 01:35:39 text_classification_dataset:239] example 0: ['There', 'have', 'been', ',', 'if', 'I', 'may', 'so', 'express', 'it', ',', 'almost', 'as', 'many', 'popular', 'as', 'royal', 'wars', '.The', 'cries', 'of', 'the', 'nation', 'and', 'the', 'importunities', 'of', 'their', 'representatives', 'have', ',', 'upon', 'various', 'occasions', ',', 'dragged', 'their', 'monarchs', 'into', 'war', ',', 'or', 'continued', 'them', 'in', 'it', ',', 'contrary', 'to', 'their', 'inclinations', ',', 'and', 'sometimes', 'contrary', 'to', 'the', 'real', 'interests', 'of', 'the', 'State', '.In', 'that', 'memorable', 'struggle', 'for', 'superiority', 'between', 'the', 'rival', 'houses', 'of', 'AUSTRIA', 'and', 'BOURBON', ',', 'which', 'so', 'long', 'kept', 'Europe', 'in', 'a', 'flame', ',', 'it', 'is', 'well', 'known', 'that', 'the', 'antipathies', 'of', 'the', 'English', 'against', 'the', 'French', ',', 'seconding', 'the', 'ambition', ',', 'or', 'rather', 'the', 'avarice', ',', 'of', 'a', 'favorite', 'leader,10', 'protracted', 'the', 'war', 'beyond', 'the', 'limits', 'marked', 'out', 'by', 'sound', 'policy', ',', 'and', 'for', 'a', 'considerable', 'time', 'in', 'opposition', 'to', 'the', 'views', 'of', 'the', 'court', '.The', 'wars', 'of', 'these', 'two', 'last-mentioned', 'nations', 'have', 'in', 'a', 'great', 'measure', 'grown', 'out', 'of', 'commercial', 'considerations', ',', '--', 'the', 'desire', 'of', 'supplanting', 'and', 'the', 'fear', 'of', 'being', 'supplanted', ',', 'either', 'in', 'particular', 'branches', 'of', 'traffic', 'or', 'in', 'the', 'general', 'advantages', 'of', 'trade', 'and', 'navigation', '.From', 'this', 'summary', 'of', 'what', 'has', 'taken', 'place', 'in', 'other', 'countries', ',', 'whose', 'situations', 'have', 'borne', 'the', 'nearest', 'resemblance', 'to', 'our', 'own', ',', 'what', 'reason', 'can', 'we', 'have', 'to', 'confide', 'in', 'those', 'reveries', 'which', 'would', 'seduce', 'us', 'into', 'an', 'expectation', 'of', 'peace', 'and', 'cordiality', 'between', 'the', 'members', 'of', 'the', 'present', 'confederacy', ',', 'in', 'a', 'state', 'of', 'separation', '?', 'Have', 'we', 'not', 'already', 'seen', 'enough', 'of', 'the', 'fallacy', 'and', 'extravagance', 'of', 'those', 'idle', 'theories', 'which', 'have', 'amused', 'us', 'with', 'promises', 'of', 'an', 'exemption', 'from', 'the', 'imperfections', ',', 'weaknesses', 'and', 'evils', 'incident', 'to', 'society', 'in', 'every', 'shape', '?']\n",
      "[NeMo I 2024-04-24 01:35:39 text_classification_dataset:240] subtokens: [CLS] there have been , if i may so express it , almost as many popular as royal wars . the cries of the nation and the import ##uni ##ties of their representatives have , upon various occasions , dragged their monarchs into war , or continued them in it , contrary to their inclination ##s , and sometimes contrary to the real interests of the state . in that memorable struggle for superiority between the rival houses of austria and bourbon , which so long kept europe in a flame , it is well known that the anti ##path ##ies of the english against the french , second ##ing the ambition , or rather the ava ##rice , of a favorite leader , 10 pro ##tracted the [SEP]\n",
      "[NeMo I 2024-04-24 01:35:39 text_classification_dataset:241] input_ids: 101 2045 2031 2042 1010 2065 1045 2089 2061 4671 2009 1010 2471 2004 2116 2759 2004 2548 5233 1012 1996 12842 1997 1996 3842 1998 1996 12324 19496 7368 1997 2037 4505 2031 1010 2588 2536 6642 1010 7944 2037 19799 2046 2162 1010 2030 2506 2068 1999 2009 1010 10043 2000 2037 21970 2015 1010 1998 2823 10043 2000 1996 2613 5426 1997 1996 2110 1012 1999 2008 13432 5998 2005 19113 2090 1996 6538 3506 1997 5118 1998 15477 1010 2029 2061 2146 2921 2885 1999 1037 8457 1010 2009 2003 2092 2124 2008 1996 3424 15069 3111 1997 1996 2394 2114 1996 2413 1010 2117 2075 1996 16290 1010 2030 2738 1996 10927 17599 1010 1997 1037 5440 3003 1010 2184 4013 24301 1996 102\n",
      "[NeMo I 2024-04-24 01:35:39 text_classification_dataset:242] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "[NeMo I 2024-04-24 01:35:39 text_classification_dataset:243] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "[NeMo I 2024-04-24 01:35:39 text_classification_dataset:244] label: 0\n",
      "[NeMo I 2024-04-24 01:35:39 text_classification_dataset:238] *** Example ***\n",
      "[NeMo I 2024-04-24 01:35:39 text_classification_dataset:239] example 1: ['They', 'would', ',', 'at', 'the', 'same', 'time', ',', 'be', 'necessitated', 'to', 'strengthen', 'the', 'executive', 'arm', 'of', 'government', ',', 'in', 'doing', 'which', 'their', 'constitutions', 'would', 'acquire', 'a', 'progressive', 'direction', 'toward', 'monarchy', '.It', 'is', 'of', 'the', 'nature', 'of', 'war', 'to', 'increase', 'the', 'executive', 'at', 'the', 'expense', 'of', 'the', 'legislative', 'authority', '.The', 'expedients', 'which', 'have', 'been', 'mentioned', 'would', 'soon', 'give', 'the', 'States', 'or', 'confederacies', 'that', 'made', 'use', 'of', 'them', 'a', 'superiority', 'over', 'their', 'neighbors', '.Small', 'states', ',', 'or', 'states', 'of', 'less', 'natural', 'strength', ',', 'under', 'vigorous', 'governments', ',', 'and', 'with', 'the', 'assistance', 'of', 'disciplined', 'armies', ',', 'have', 'often', 'triumphed', 'over', 'large', 'states', ',', 'or', 'states', 'of', 'greater', 'natural', 'strength', ',', 'which', 'have', 'been', 'destitute', 'of', 'these', 'advantages', '.Neither', 'the', 'pride', 'nor', 'the', 'safety', 'of', 'the', 'more', 'important', 'States', 'or', 'confederacies', 'would', 'permit', 'them', 'long', 'to', 'submit', 'to', 'this', 'mortifying', 'and', 'adventitious', 'superiority', '.They', 'would', 'quickly', 'resort', 'to', 'means', 'similar', 'to', 'those', 'by', 'which', 'it', 'had', 'been', 'effected', ',', 'to', 'reinstate', 'themselves', 'in', 'their', 'lost', 'pre-eminence', '.Thus', ',', 'we', 'should', ',', 'in', 'a', 'little', 'time', ',', 'see', 'established', 'in', 'every', 'part', 'of', 'this', 'country', 'the', 'same', 'engines', 'of', 'despotism', 'which', 'have', 'been', 'the', 'scourge', 'of', 'the', 'Old', 'World', '.This', ',', 'at', 'least', ',', 'would', 'be', 'the', 'natural', 'course', 'of', 'things', ';', 'and', 'our', 'reasonings', 'will', 'be', 'the', 'more', 'likely', 'to', 'be', 'just', ',', 'in', 'proportion', 'as', 'they', 'are', 'accommodated', 'to', 'this', 'standard', '.These', 'are', 'not', 'vague', 'inferences', 'drawn', 'from', 'supposed', 'or', 'speculative', 'defects', 'in', 'a', 'Constitution', ',', 'the', 'whole', 'power', 'of', 'which', 'is', 'lodged', 'in', 'the', 'hands', 'of', 'a', 'people', ',', 'or', 'their', 'representatives', 'and', 'delegates', ',', 'but', 'they', 'are', 'solid', 'conclusions', ',', 'drawn', 'from', 'the', 'natural', 'and', 'necessary', 'progress', 'of', 'human', 'affairs', '.']\n",
      "[NeMo I 2024-04-24 01:35:39 text_classification_dataset:240] subtokens: [CLS] they would , at the same time , be necessitated to strengthen the executive arm of government , in doing which their constitution ##s would acquire a progressive direction toward monarchy . it is of the nature of war to increase the executive at the expense of the legislative authority . the ex ##ped ##ient ##s which have been mentioned would soon give the states or con ##fed ##era ##cies that made use of them a superiority over their neighbors . small states , or states of less natural strength , under vigorous governments , and with the assistance of disciplined armies , have often triumph ##ed over large states , or states of greater natural strength , which have been des ##ti ##tute of these advantages [SEP]\n",
      "[NeMo I 2024-04-24 01:35:39 text_classification_dataset:241] input_ids: 101 2027 2052 1010 2012 1996 2168 2051 1010 2022 29611 2000 12919 1996 3237 2849 1997 2231 1010 1999 2725 2029 2037 4552 2015 2052 9878 1037 6555 3257 2646 12078 1012 2009 2003 1997 1996 3267 1997 2162 2000 3623 1996 3237 2012 1996 10961 1997 1996 4884 3691 1012 1996 4654 5669 11638 2015 2029 2031 2042 3855 2052 2574 2507 1996 2163 2030 9530 25031 6906 9243 2008 2081 2224 1997 2068 1037 19113 2058 2037 10638 1012 2235 2163 1010 2030 2163 1997 2625 3019 3997 1010 2104 21813 6867 1010 1998 2007 1996 5375 1997 28675 8749 1010 2031 2411 10911 2098 2058 2312 2163 1010 2030 2163 1997 3618 3019 3997 1010 2029 2031 2042 4078 3775 24518 1997 2122 12637 102\n",
      "[NeMo I 2024-04-24 01:35:39 text_classification_dataset:242] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "[NeMo I 2024-04-24 01:35:39 text_classification_dataset:243] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "[NeMo I 2024-04-24 01:35:39 text_classification_dataset:244] label: 0\n",
      "[NeMo W 2024-04-24 01:35:41 text_classification_dataset:250] Found 115 out of 115 sentences with more than 128 subtokens. Truncated long sentences from the end.\n",
      "[NeMo I 2024-04-24 01:35:41 data_preprocessing:299] Some stats of the lengths of the sequences:\n",
      "[NeMo I 2024-04-24 01:35:41 data_preprocessing:301] Min: 129 |                  Max: 129 |                  Mean: 129.0 |                  Median: 129.0\n",
      "[NeMo I 2024-04-24 01:35:41 data_preprocessing:307] 75 percentile: 129.00\n",
      "[NeMo I 2024-04-24 01:35:41 data_preprocessing:308] 99 percentile: 129.00\n",
      "[NeMo I 2024-04-24 01:35:41 text_classification_model:216] Dataloader config or file_path for the test is missing, so no data loader for test is created!\n",
      "[NeMo W 2024-04-24 01:35:41 modelPT:197] You tried to register an artifact under config key=tokenizer.vocab_file but an artifact forit has already been registered.\n",
      "[NeMo W 2024-04-24 01:35:41 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/nemo/core/classes/modelPT.py:243: UserWarning: update_node() is deprecated, use OmegaConf.update(). (Since 2.0)\n",
      "      self.cfg.update_node(config_path, return_path)\n",
      "    \n",
      "Lock 139819841352080 acquired on /root/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f.lock\n",
      "Downloading: 100%|███████████████████████████| 440M/440M [00:05<00:00, 85.4MB/s]\n",
      "Lock 139819841352080 released on /root/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f.lock\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertEncoder: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[NeMo I 2024-04-24 01:35:49 text_classification_with_bert:118] ===========================================================================================\n",
      "[NeMo I 2024-04-24 01:35:49 text_classification_with_bert:119] Starting training...\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "[NeMo I 2024-04-24 01:35:49 modelPT:748] Optimizer config = Adam (\n",
      "    Parameter Group 0\n",
      "        amsgrad: False\n",
      "        betas: [0.9, 0.999]\n",
      "        eps: 1e-08\n",
      "        lr: 2e-05\n",
      "        weight_decay: 0.01\n",
      "    )\n",
      "[NeMo I 2024-04-24 01:35:49 lr_scheduler:617] Scheduler \"<nemo.core.optim.lr_scheduler.WarmupAnnealing object at 0x7f2a57f26be0>\" \n",
      "    will be used during training (effective maximum steps = 320) - \n",
      "    Parameters : \n",
      "    (warmup_steps: null\n",
      "    warmup_ratio: 0.1\n",
      "    last_epoch: -1\n",
      "    max_steps: 320\n",
      "    )\n",
      "initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1\n",
      "Added key: store_based_barrier_key:1 to store for rank: 0\n",
      "\n",
      "  | Name                  | Type                 | Params\n",
      "---------------------------------------------------------------\n",
      "0 | loss                  | CrossEntropyLoss     | 0     \n",
      "1 | bert_model            | BertEncoder          | 109 M \n",
      "2 | classifier            | SequenceClassifier   | 592 K \n",
      "3 | classification_report | ClassificationReport | 0     \n",
      "---------------------------------------------------------------\n",
      "110 M     Trainable params\n",
      "0         Non-trainable params\n",
      "110 M     Total params\n",
      "440.297   Total estimated model params size (MB)\n",
      "Epoch 0:  82%|▊| 33/40 [00:03<00:00,  9.02it/s, loss=0.629, v_num=5-30, lr=1.88e\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:  88%|▉| 35/40 [00:03<00:00,  9.24it/s, loss=0.629, v_num=5-30, lr=1.88e\u001b[A\n",
      "Epoch 0:  98%|▉| 39/40 [00:03<00:00, 10.02it/s, loss=0.629, v_num=5-30, lr=1.88e\u001b[A[NeMo I 2024-04-24 01:35:55 text_classification_model:165] val_report: \n",
      "    label                                                precision    recall       f1           support   \n",
      "    label_id: 0                                             77.39     100.00      87.25         89\n",
      "    label_id: 1                                              0.00       0.00       0.00         26\n",
      "    -------------------\n",
      "    micro avg                                               77.39      77.39      77.39        115\n",
      "    macro avg                                               38.70      50.00      43.63        115\n",
      "    weighted avg                                            59.89      77.39      67.53        115\n",
      "    \n",
      "Epoch 0: 100%|█| 40/40 [00:03<00:00, 10.07it/s, loss=0.629, v_num=5-30, lr=1.94e\n",
      "                                                                                \u001b[AEpoch 0, global step 31: val_loss reached 0.61619 (best 0.61619), saving model to \"/dli/task/nemo_experiments/TextClassification/2024-04-24_01-35-30/checkpoints/TextClassification--val_loss=0.62-epoch=0.ckpt\" as top 3\n",
      "Epoch 1:  80%|▊| 32/40 [00:03<00:00, 10.29it/s, loss=0.59, v_num=5-30, lr=1.79e-\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 1:  90%|▉| 36/40 [00:03<00:00, 11.00it/s, loss=0.59, v_num=5-30, lr=1.79e-\u001b[A\n",
      "Epoch 1: 100%|█| 40/40 [00:03<00:00, 11.84it/s, loss=0.59, v_num=5-30, lr=1.79e-\u001b[A[NeMo I 2024-04-24 01:36:02 text_classification_model:165] val_report: \n",
      "    label                                                precision    recall       f1           support   \n",
      "    label_id: 0                                             77.39     100.00      87.25         89\n",
      "    label_id: 1                                              0.00       0.00       0.00         26\n",
      "    -------------------\n",
      "    micro avg                                               77.39      77.39      77.39        115\n",
      "    macro avg                                               38.70      50.00      43.63        115\n",
      "    weighted avg                                            59.89      77.39      67.53        115\n",
      "    \n",
      "Epoch 1: 100%|█| 40/40 [00:03<00:00, 11.69it/s, loss=0.59, v_num=5-30, lr=1.78e-\n",
      "                                                                                \u001b[AEpoch 1, global step 63: val_loss reached 0.61353 (best 0.61353), saving model to \"/dli/task/nemo_experiments/TextClassification/2024-04-24_01-35-30/checkpoints/TextClassification--val_loss=0.61-epoch=1.ckpt\" as top 3\n",
      "Epoch 2:  80%|▊| 32/40 [00:03<00:00, 10.25it/s, loss=0.556, v_num=5-30, lr=1.57e\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 2:  90%|▉| 36/40 [00:03<00:00, 10.98it/s, loss=0.556, v_num=5-30, lr=1.57e\u001b[A\n",
      "Epoch 2: 100%|█| 40/40 [00:03<00:00, 11.82it/s, loss=0.556, v_num=5-30, lr=1.57e\u001b[A[NeMo I 2024-04-24 01:36:09 text_classification_model:165] val_report: \n",
      "    label                                                precision    recall       f1           support   \n",
      "    label_id: 0                                             77.39     100.00      87.25         89\n",
      "    label_id: 1                                              0.00       0.00       0.00         26\n",
      "    -------------------\n",
      "    micro avg                                               77.39      77.39      77.39        115\n",
      "    macro avg                                               38.70      50.00      43.63        115\n",
      "    weighted avg                                            59.89      77.39      67.53        115\n",
      "    \n",
      "Epoch 2: 100%|█| 40/40 [00:03<00:00, 11.67it/s, loss=0.556, v_num=5-30, lr=1.56e\n",
      "                                                                                \u001b[AEpoch 2, global step 95: val_loss reached 0.57206 (best 0.57206), saving model to \"/dli/task/nemo_experiments/TextClassification/2024-04-24_01-35-30/checkpoints/TextClassification--val_loss=0.57-epoch=2.ckpt\" as top 3\n",
      "Epoch 3:  80%|▊| 32/40 [00:03<00:00, 10.24it/s, loss=0.502, v_num=5-30, lr=1.35e\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 3:  90%|▉| 36/40 [00:03<00:00, 10.93it/s, loss=0.502, v_num=5-30, lr=1.35e\u001b[A\n",
      "Epoch 3: 100%|█| 40/40 [00:03<00:00, 11.77it/s, loss=0.502, v_num=5-30, lr=1.35e[NeMo I 2024-04-24 01:36:17 text_classification_model:165] val_report: \n",
      "    label                                                precision    recall       f1           support   \n",
      "    label_id: 0                                             80.00      98.88      88.44         89\n",
      "    label_id: 1                                             80.00      15.38      25.81         26\n",
      "    -------------------\n",
      "    micro avg                                               80.00      80.00      80.00        115\n",
      "    macro avg                                               80.00      57.13      57.12        115\n",
      "    weighted avg                                            80.00      80.00      74.28        115\n",
      "    \n",
      "Epoch 3: 100%|█| 40/40 [00:03<00:00, 11.63it/s, loss=0.502, v_num=5-30, lr=1.34e\n",
      "                                                                                \u001b[AEpoch 3, global step 127: val_loss reached 0.52154 (best 0.52154), saving model to \"/dli/task/nemo_experiments/TextClassification/2024-04-24_01-35-30/checkpoints/TextClassification--val_loss=0.52-epoch=3.ckpt\" as top 3\n",
      "Epoch 4:  80%|▊| 32/40 [00:03<00:00, 10.38it/s, loss=0.454, v_num=5-30, lr=1.13e\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 4:  90%|▉| 36/40 [00:03<00:00, 11.07it/s, loss=0.454, v_num=5-30, lr=1.13e\u001b[A\n",
      "Epoch 4: 100%|█| 40/40 [00:03<00:00, 11.93it/s, loss=0.454, v_num=5-30, lr=1.13e\u001b[A[NeMo I 2024-04-24 01:36:24 text_classification_model:165] val_report: \n",
      "    label                                                precision    recall       f1           support   \n",
      "    label_id: 0                                             80.00      94.38      86.60         89\n",
      "    label_id: 1                                             50.00      19.23      27.78         26\n",
      "    -------------------\n",
      "    micro avg                                               77.39      77.39      77.39        115\n",
      "    macro avg                                               65.00      56.81      57.19        115\n",
      "    weighted avg                                            73.22      77.39      73.30        115\n",
      "    \n",
      "Epoch 4: 100%|█| 40/40 [00:03<00:00, 11.76it/s, loss=0.454, v_num=5-30, lr=1.12e\n",
      "                                                                                \u001b[AEpoch 4, global step 159: val_loss reached 0.49344 (best 0.49344), saving model to \"/dli/task/nemo_experiments/TextClassification/2024-04-24_01-35-30/checkpoints/TextClassification--val_loss=0.49-epoch=4.ckpt\" as top 3\n",
      "Epoch 5:  80%|▊| 32/40 [00:03<00:00, 10.42it/s, loss=0.316, v_num=5-30, lr=9.03e\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 5:  90%|▉| 36/40 [00:03<00:00, 11.15it/s, loss=0.316, v_num=5-30, lr=9.03e\u001b[A\n",
      "Epoch 5: 100%|█| 40/40 [00:03<00:00, 11.99it/s, loss=0.316, v_num=5-30, lr=9.03e\u001b[A[NeMo I 2024-04-24 01:36:32 text_classification_model:165] val_report: \n",
      "    label                                                precision    recall       f1           support   \n",
      "    label_id: 0                                             81.48      98.88      89.34         89\n",
      "    label_id: 1                                             85.71      23.08      36.36         26\n",
      "    -------------------\n",
      "    micro avg                                               81.74      81.74      81.74        115\n",
      "    macro avg                                               83.60      60.98      62.85        115\n",
      "    weighted avg                                            82.44      81.74      77.36        115\n",
      "    \n",
      "Epoch 5: 100%|█| 40/40 [00:03<00:00, 11.84it/s, loss=0.316, v_num=5-30, lr=8.96e\n",
      "                                                                                \u001b[AEpoch 5, step 191: val_loss was not in top 3\n",
      "Epoch 6:  80%|▊| 32/40 [00:03<00:00, 10.26it/s, loss=0.227, v_num=5-30, lr=6.81e\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 6:  90%|▉| 36/40 [00:03<00:00, 10.96it/s, loss=0.227, v_num=5-30, lr=6.81e\u001b[A\n",
      "Epoch 6: 100%|█| 40/40 [00:03<00:00, 11.80it/s, loss=0.227, v_num=5-30, lr=6.81e\u001b[A[NeMo I 2024-04-24 01:36:37 text_classification_model:165] val_report: \n",
      "    label                                                precision    recall       f1           support   \n",
      "    label_id: 0                                             83.84      93.26      88.30         89\n",
      "    label_id: 1                                             62.50      38.46      47.62         26\n",
      "    -------------------\n",
      "    micro avg                                               80.87      80.87      80.87        115\n",
      "    macro avg                                               73.17      65.86      67.96        115\n",
      "    weighted avg                                            79.01      80.87      79.10        115\n",
      "    \n",
      "Epoch 6: 100%|█| 40/40 [00:03<00:00, 11.65it/s, loss=0.227, v_num=5-30, lr=6.74e\n",
      "                                                                                \u001b[AEpoch 6, global step 223: val_loss reached 0.39491 (best 0.39491), saving model to \"/dli/task/nemo_experiments/TextClassification/2024-04-24_01-35-30/checkpoints/TextClassification--val_loss=0.39-epoch=6.ckpt\" as top 3\n",
      "Epoch 7:  80%|▊| 32/40 [00:03<00:00, 10.29it/s, loss=0.134, v_num=5-30, lr=4.58e\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 7:  90%|▉| 36/40 [00:03<00:00, 10.99it/s, loss=0.134, v_num=5-30, lr=4.58e\u001b[A\n",
      "Epoch 7: 100%|█| 40/40 [00:03<00:00, 11.84it/s, loss=0.134, v_num=5-30, lr=4.58e\u001b[A[NeMo I 2024-04-24 01:36:45 text_classification_model:165] val_report: \n",
      "    label                                                precision    recall       f1           support   \n",
      "    label_id: 0                                             95.24      89.89      92.49         89\n",
      "    label_id: 1                                             70.97      84.62      77.19         26\n",
      "    -------------------\n",
      "    micro avg                                               88.70      88.70      88.70        115\n",
      "    macro avg                                               83.10      87.25      84.84        115\n",
      "    weighted avg                                            89.75      88.70      89.03        115\n",
      "    \n",
      "Epoch 7: 100%|█| 40/40 [00:03<00:00, 11.66it/s, loss=0.134, v_num=5-30, lr=4.51e\n",
      "                                                                                \u001b[AEpoch 7, global step 255: val_loss reached 0.31508 (best 0.31508), saving model to \"/dli/task/nemo_experiments/TextClassification/2024-04-24_01-35-30/checkpoints/TextClassification--val_loss=0.32-epoch=7.ckpt\" as top 3\n",
      "Epoch 8:  80%|▊| 32/40 [00:03<00:00, 10.35it/s, loss=0.118, v_num=5-30, lr=2.36e\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 8:  90%|▉| 36/40 [00:03<00:00, 11.07it/s, loss=0.118, v_num=5-30, lr=2.36e\u001b[A\n",
      "Epoch 8: 100%|█| 40/40 [00:03<00:00, 11.92it/s, loss=0.118, v_num=5-30, lr=2.36e\u001b[A[NeMo I 2024-04-24 01:36:52 text_classification_model:165] val_report: \n",
      "    label                                                precision    recall       f1           support   \n",
      "    label_id: 0                                             82.69      96.63      89.12         89\n",
      "    label_id: 1                                             72.73      30.77      43.24         26\n",
      "    -------------------\n",
      "    micro avg                                               81.74      81.74      81.74        115\n",
      "    macro avg                                               77.71      63.70      66.18        115\n",
      "    weighted avg                                            80.44      81.74      78.75        115\n",
      "    \n",
      "Epoch 8: 100%|█| 40/40 [00:03<00:00, 11.76it/s, loss=0.118, v_num=5-30, lr=2.29e\n",
      "                                                                                \u001b[AEpoch 8, global step 287: val_loss reached 0.49101 (best 0.31508), saving model to \"/dli/task/nemo_experiments/TextClassification/2024-04-24_01-35-30/checkpoints/TextClassification--val_loss=0.49-epoch=8.ckpt\" as top 3\n",
      "Epoch 9:  80%|▊| 32/40 [00:03<00:00, 10.21it/s, loss=0.0795, v_num=5-30, lr=1.39\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 9:  90%|▉| 36/40 [00:03<00:00, 10.91it/s, loss=0.0795, v_num=5-30, lr=1.39\u001b[A\n",
      "Epoch 9: 100%|█| 40/40 [00:03<00:00, 11.75it/s, loss=0.0795, v_num=5-30, lr=1.39\u001b[A[NeMo I 2024-04-24 01:37:00 text_classification_model:165] val_report: \n",
      "    label                                                precision    recall       f1           support   \n",
      "    label_id: 0                                             91.11      92.13      91.62         89\n",
      "    label_id: 1                                             72.00      69.23      70.59         26\n",
      "    -------------------\n",
      "    micro avg                                               86.96      86.96      86.96        115\n",
      "    macro avg                                               81.56      80.68      81.10        115\n",
      "    weighted avg                                            86.79      86.96      86.87        115\n",
      "    \n",
      "Epoch 9: 100%|█| 40/40 [00:03<00:00, 11.60it/s, loss=0.0795, v_num=5-30, lr=6.94\n",
      "                                                                                \u001b[AEpoch 9, global step 319: val_loss reached 0.32788 (best 0.31508), saving model to \"/dli/task/nemo_experiments/TextClassification/2024-04-24_01-35-30/checkpoints/TextClassification--val_loss=0.33-epoch=9.ckpt\" as top 3\n",
      "Saving latest checkpoint...\n",
      "Epoch 9: 100%|█| 40/40 [00:07<00:00,  5.40it/s, loss=0.0795, v_num=5-30, lr=6.94\n",
      "[NeMo W 2024-04-24 01:37:04 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/nemo/core/classes/modelPT.py:308: UserWarning: update_node() is deprecated, use OmegaConf.update(). (Since 2.0)\n",
      "      conf.update_node(conf_path, item.path)\n",
      "    \n",
      "[NeMo I 2024-04-24 01:37:26 text_classification_with_bert:121] Training finished!\n",
      "[NeMo I 2024-04-24 01:37:26 text_classification_with_bert:122] ===========================================================================================\n",
      "[NeMo I 2024-04-24 01:37:48 text_classification_with_bert:127] Model is saved into `.nemo` file: text_classification_model.nemo\n",
      "CPU times: user 1.42 s, sys: 690 ms, total: 2.11 s\n",
      "Wall time: 2min 21s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Run the training script, overriding the config values in the command line\n",
    "TC_DIR = \"/dli/task/nemo/examples/nlp/text_classification\"\n",
    "\n",
    "\n",
    "!python $TC_DIR/text_classification_with_bert.py \\\n",
    "        model.dataset.num_classes=$NUM_CLASSES \\\n",
    "        model.dataset.max_seq_length=$MAX_SEQ_LENGTH \\\n",
    "        model.train_ds.file_path=$PATH_TO_TRAIN_FILE \\\n",
    "        model.validation_ds.file_path=$PATH_TO_VAL_FILE \\\n",
    "        model.infer_samples=[] \\\n",
    "        trainer.max_epochs=$MAX_EPOCHS \\\n",
    "        model.language_model.pretrained_model_name=$PRETRAINED_MODEL_NAME \\\n",
    "        trainer.amp_level=$AMP_LEVEL \\\n",
    "        trainer.precision=$PRECISION \\\n",
    "        model.train_ds.batch_size=$BATCH_SIZE \\\n",
    "        model.validation_ds.batch_size=$BATCH_SIZE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run to save for assessment- DO NOT CHANGE\n",
    "cmd_log = os.path.join(os.path.dirname(os.path.dirname(get_latest_model())),'cmd-args.log')\n",
    "lightning_logs = os.path.join(os.path.dirname(os.path.dirname(get_latest_model())),'lightning_logs.txt')\n",
    "\n",
    "with open(cmd_log, \"r\") as f:\n",
    "    cmd = f.read()\n",
    "    cmd_list = cmd.split()\n",
    "with open(\"my_assessment/step4.json\", \"w\") as outfile: \n",
    "    json.dump(cmd_list, outfile) \n",
    "    \n",
    "with open(lightning_logs, \"r\") as f:\n",
    "    log = f.readlines()\n",
    "with open(\"my_assessment/step4_lightning.json\", \"w\") as outfile:\n",
    "    json.dump(log, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 步驟 5：推論"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 執行推論 (計分)\n",
    "執行推論區塊以查看並儲存結果。(注意：此處沒有任何問題需要修正)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using bos_token, but it is not set yet.\n",
      "Using eos_token, but it is not set yet.\n",
      "[NeMo W 2024-04-24 01:38:11 modelPT:137] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
      "    Train config : \n",
      "    file_path: /dli/task/data/federalist_papers_HM/train_nemo_format.tsv\n",
      "    batch_size: 16\n",
      "    shuffle: true\n",
      "    num_samples: -1\n",
      "    num_workers: 3\n",
      "    drop_last: false\n",
      "    pin_memory: false\n",
      "    \n",
      "[NeMo W 2024-04-24 01:38:11 modelPT:144] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
      "    Validation config : \n",
      "    file_path: /dli/task/data/federalist_papers_HM/dev_nemo_format.tsv\n",
      "    batch_size: 16\n",
      "    shuffle: false\n",
      "    num_samples: -1\n",
      "    num_workers: 3\n",
      "    drop_last: false\n",
      "    pin_memory: false\n",
      "    \n",
      "[NeMo W 2024-04-24 01:38:11 modelPT:151] Please call the ModelPT.setup_test_data() or ModelPT.setup_multiple_test_data() method and provide a valid configuration file to setup the test data loader(s).\n",
      "    Test config : \n",
      "    file_path: null\n",
      "    batch_size: 64\n",
      "    shuffle: false\n",
      "    num_samples: -1\n",
      "    num_workers: 3\n",
      "    drop_last: false\n",
      "    pin_memory: false\n",
      "    \n",
      "[NeMo W 2024-04-24 01:38:11 modelPT:1198] World size can only be set by PyTorch Lightning Trainer.\n",
      "[NeMo W 2024-04-24 01:38:11 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/nemo/core/classes/modelPT.py:243: UserWarning: update_node() is deprecated, use OmegaConf.update(). (Since 2.0)\n",
      "      self.cfg.update_node(config_path, return_path)\n",
      "    \n",
      "[NeMo W 2024-04-24 01:38:11 modelPT:197] You tried to register an artifact under config key=tokenizer.vocab_file but an artifact forit has already been registered.\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-04-24 01:38:15 modelPT:434] Model TextClassificationModel was successfully restored from nemo_experiments/TextClassification/2024-04-24_01-35-30/checkpoints/TextClassification.nemo.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-04-24 01:38:15 text_classification_dataset:250] Found 7 out of 7 sentences with more than 256 subtokens. Truncated long sentences from the end.\n",
      "[NeMo W 2024-04-24 01:38:16 text_classification_dataset:250] Found 4 out of 4 sentences with more than 256 subtokens. Truncated long sentences from the end.\n",
      "[NeMo W 2024-04-24 01:38:16 text_classification_dataset:250] Found 7 out of 8 sentences with more than 256 subtokens. Truncated long sentences from the end.\n",
      "[NeMo W 2024-04-24 01:38:16 text_classification_dataset:250] Found 7 out of 7 sentences with more than 256 subtokens. Truncated long sentences from the end.\n",
      "[NeMo W 2024-04-24 01:38:17 text_classification_dataset:250] Found 9 out of 9 sentences with more than 256 subtokens. Truncated long sentences from the end.\n",
      "[NeMo W 2024-04-24 01:38:17 text_classification_dataset:250] Found 8 out of 8 sentences with more than 256 subtokens. Truncated long sentences from the end.\n",
      "[NeMo W 2024-04-24 01:38:17 text_classification_dataset:250] Found 8 out of 8 sentences with more than 256 subtokens. Truncated long sentences from the end.\n",
      "[NeMo W 2024-04-24 01:38:18 text_classification_dataset:250] Found 6 out of 6 sentences with more than 256 subtokens. Truncated long sentences from the end.\n",
      "[NeMo W 2024-04-24 01:38:18 text_classification_dataset:250] Found 9 out of 9 sentences with more than 256 subtokens. Truncated long sentences from the end.\n",
      "[NeMo W 2024-04-24 01:38:18 text_classification_dataset:250] Found 22 out of 22 sentences with more than 256 subtokens. Truncated long sentences from the end.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 1, 0, 1, 1, 1, 0], [1, 1, 1, 1], [1, 0, 0, 1, 1, 1, 1, 0], [1, 1, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 1, 1, 1, 1], [0, 1, 1, 1, 1, 1, 1, 1], [1, 1, 0, 1, 1, 1, 1, 0], [0, 1, 1, 1, 1, 0], [1, 1, 0, 0, 1, 1, 0, 1, 0], [0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0]]\n"
     ]
    }
   ],
   "source": [
    "# Run inference for assessment -  - DO NOT CHANGE\n",
    "from nemo.collections import nlp as nemo_nlp\n",
    "\n",
    "# Instantiate the model by restoring from the latest .nemo checkpoint\n",
    "model = nemo_nlp.models.TextClassificationModel.restore_from(get_latest_model())\n",
    "\n",
    "# Find the latest model path\n",
    "DATA_DIR = '/dli/task/data/federalist_papers_HM'\n",
    "\n",
    "test_files = [\n",
    "    'test49.tsv',\n",
    "    'test50.tsv',\n",
    "    'test51.tsv',\n",
    "    'test52.tsv',\n",
    "    'test53.tsv',\n",
    "    'test54.tsv', \n",
    "    'test55.tsv',\n",
    "    'test56.tsv',\n",
    "    'test57.tsv',\n",
    "    'test62.tsv',\n",
    "]\n",
    "results = []\n",
    "for test_file in test_files:\n",
    "    # get as list and remove header row\n",
    "    filepath = os.path.join(DATA_DIR, test_file)\n",
    "    with open(filepath, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "    del lines[0]\n",
    "    \n",
    "    results.append(model.classifytext(lines, batch_size = 1, max_seq_length = 256))\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MADISON\n",
      "MADISON\n",
      "MADISON\n",
      "HAMILTON\n",
      "MADISON\n",
      "MADISON\n",
      "MADISON\n",
      "MADISON\n",
      "MADISON\n",
      "HAMILTON\n"
     ]
    }
   ],
   "source": [
    "# Run to save for assessment- DO NOT CHANGE\n",
    "author = []\n",
    "for result in results:\n",
    "    avg_result = sum(result) / len(result)\n",
    "    if avg_result < 0.5:\n",
    "        author.append(\"HAMILTON\")\n",
    "        print(\"HAMILTON\")\n",
    "    else:\n",
    "        author.append(\"MADISON\")\n",
    "        print(\"MADISON\")\n",
    "        \n",
    "with open(\"my_assessment/step5.json\", \"w\") as outfile: \n",
    "    json.dump(author, outfile) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 步驟 6：提交測驗\n",
    "結果如何？根據先前的[運用支援向量機器完成機器學習分析](http://pages.cs.wisc.edu/~gfung/federalist.pdf)，Madison 最有可能是全部有作者身分爭議文章的真實作者 (假設沒有合著的情況)。您使用的工具很可能得出「全部為 MADISON」的答案。如果您堅持，可以繼續嘗試，雖然**要通過測驗*無需*取得特定結果**。\n",
    "\n",
    "如果您認為已正確完成程式碼，且訓練與推論也正確運作，可以按照下列方式將專案提交至自動評分工具：\n",
    "\n",
    "1.返回 GPU 啟動頁面，並按一下核取符號以執行測驗：\n",
    "\n",
    "<img src=\"../images/assessment_checkmark.png\" width=600>\n",
    "\n",
    "2.就是這麼簡單！如果您通過，會出現彈出式視窗，說明您已通過，且分數會累積至您的進度。若您未通過，會在彈出式視窗中獲得意見回饋。\n",
    "\n",
    "<img src=\"../images/assessment_pass_popup.png\" width=600>\n",
    "\n",
    "您可以隨時在課程進度分頁中查看自己的測驗進度。請注意：編碼測驗各部分的分數值不會出現於此處，這裡只會顯示為 0 分或 70 分。請務必在同一課程頁面中完成 Transformer 與部署的問題，才符合取得最終認證證書的資格！\n",
    "\n",
    "<img src=\"../images/progress.png\" width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"../images/DLI_Header.png\" alt=\"標頭\" style=\"width: 400px;\"/> </a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
